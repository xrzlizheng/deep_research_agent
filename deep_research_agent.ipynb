{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e67d5fef-280b-4147-936b-6bfdebe132ed",
   "metadata": {},
   "source": [
    "# Deep Research Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd594550-b76f-492e-bb93-6b8e2d536108",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8168e6f4-5015-4d85-a167-28fc278fcb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import json\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "from tavily import TavilyClient\n",
    "from json.decoder import JSONDecodeError\n",
    "from pydantic_settings import BaseSettings\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527271f6-8edc-44c2-913b-a808415a67e2",
   "metadata": {},
   "source": [
    "### Constructing application Configuration object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f1ab7e-8161-4f83-969f-8d6be94d4b0a",
   "metadata": {},
   "source": [
    "Note that we have two LLMs configures, we will be using a reasoning model (DeepSeek-R1) for some of the sub agents while we will be using a regular instruction tuned Llama model (Meta-Llama-3.3-70B-Instruct) for other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2693c91-4081-4839-8c8c-3bc9c243e762",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(BaseSettings):\n",
    "    SAMBANOVA_API_KEY: str\n",
    "    SAMBANOVA_BASE_URL: str\n",
    "    LLM_REASONING: str\n",
    "    LLM_REGULAR: str\n",
    "    TAVILY_API_KEY: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87a818b-31f4-41a5-99ae-29f17208ec18",
   "metadata": {},
   "source": [
    "Be sure to have your SAMBANOVA_API_KEY (get it [here](https://fnf.dev/4aVUqro)) and TAVILY_API_KEY (get it [here](https://app.tavily.com/)) exported as environment variables before running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd56f21-55ad-483b-af91-3a5dfa5c2225",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(SAMBANOVA_API_KEY=os.environ[\"SAMBANOVA_API_KEY\"],\n",
    "                SAMBANOVA_BASE_URL=\"https://api.sambanova.ai/v1\",\n",
    "                LLM_REASONING=\"DeepSeek-R1-Distill-Llama-70B\",\n",
    "                LLM_REGULAR=\"Meta-Llama-3.3-70B-Instruct\",\n",
    "                TAVILY_API_KEY=os.environ[\"TAVILY_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7133be4d-524d-40b8-91fc-53fb89cc6067",
   "metadata": {},
   "source": [
    "### Data Classes to define the System State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d38df5-5c0e-47b7-902b-8b9c36cc7e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Search:\n",
    "    url: str = \"\"\n",
    "    content: str = \"\"\n",
    "\n",
    "@dataclass\n",
    "class Research:\n",
    "    search_history: List[Search] = field(default_factory=list)\n",
    "    latest_summary: str = \"\"\n",
    "    reflection_iteration: int = 0\n",
    "\n",
    "@dataclass\n",
    "class Paragraph:\n",
    "    title: str = \"\"\n",
    "    content: str = \"\"\n",
    "    research: Research = field(default_factory=Research)\n",
    "\n",
    "@dataclass\n",
    "class State:\n",
    "    report_title: str = \"\"\n",
    "    paragraphs: List[Paragraph] = field(default_factory=list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f226607-50ca-4dfc-b476-38f365857fee",
   "metadata": {},
   "source": [
    "### Helper functions for data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b435be4-2a9b-424e-9623-1926170e8e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_reasoning_from_output(output):\n",
    "    return output.split(\"</think>\")[-1].strip()\n",
    "\n",
    "def clean_json_tags(text):\n",
    "    return text.replace(\"```json\\n\", \"\").replace(\"\\n```\", \"\")\n",
    "\n",
    "def clean_markdown_tags(text):\n",
    "    return text.replace(\"```markdown\\n\", \"\").replace(\"\\n```\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33a32ff-7c41-45c8-829b-44a0a4045a44",
   "metadata": {},
   "source": [
    "### Search tool and a fuction to update System State with search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc6631b-66a1-41b4-95ee-59656fc9567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tavily_search(query, include_raw_content=True, max_results=3):\n",
    "\n",
    "    tavily_client = TavilyClient(api_key=config.TAVILY_API_KEY)\n",
    "\n",
    "    return tavily_client.search(query,\n",
    "                                include_raw_content=include_raw_content,\n",
    "                                max_results=max_results)\n",
    "\n",
    "def update_state_with_search_results(search_results, idx_paragraph, state):\n",
    "    \n",
    "    for search_result in search_results[\"results\"]:\n",
    "        search = Search(url=search_result[\"url\"], content=search_result[\"raw_content\"])\n",
    "        state.paragraphs[idx_paragraph].research.search_history.append(search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a8ec04-f822-4c9f-8a7d-b8e2836c9358",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d6e379-4c23-4e44-ada2-b712165aa7b3",
   "metadata": {},
   "source": [
    "Here we define LLM sub-Agents that will read the System State, perform computation and evolve the state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40f5af5-a137-44cb-acb3-13f8df8597f9",
   "metadata": {},
   "source": [
    "### Agent for Report structure creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e2bc1b-c315-4c78-a7f1-5b4f8df93d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_schema_report_structure = {\n",
    "        \"type\": \"array\",\n",
    "        \"items\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"title\": {\"type\": \"string\"},\n",
    "                \"content\": {\"type\": \"string\"}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "SYSTEM_PROMPT_REPORT_STRUCTURE = f\"\"\"\n",
    "You are a Deep Research assistan. Given a query, plan a structure for a report and the paragraphs to be included.\n",
    "Make sure that the ordering of paragraphs makes sense.\n",
    "Once the outline is created, you will be given tools to search the web and reflect for each of the section separately.\n",
    "Format the output in json with the following json schema definition:\n",
    "\n",
    "<OUTPUT JSON SCHEMA>\n",
    "{json.dumps(output_schema_report_structure, indent=2)}\n",
    "</OUTPUT JSON SCHEMA>\n",
    "\n",
    "Title and content properties will be used for deeper research.\n",
    "Make sure that the output is a json object with an output json schema defined above.\n",
    "Only return the json object, no explanation or additional text.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86944c78-5fb2-4bde-9325-dba96d06ca27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReportStructureAgent:\n",
    "\n",
    "    def __init__(self, query: str):\n",
    "\n",
    "        self.openai_client = openai.OpenAI(\n",
    "            api_key=config.SAMBANOVA_API_KEY,\n",
    "            base_url=config.SAMBANOVA_BASE_URL\n",
    "        )\n",
    "        self.query = query\n",
    "\n",
    "    def run(self) -> str:\n",
    "\n",
    "        response = self.openai_client.chat.completions.create(\n",
    "            model=config.LLM_REASONING,\n",
    "            messages=[{\"role\": \"system\", \"content\": SYSTEM_PROMPT_REPORT_STRUCTURE},\n",
    "                      {\"role\":\"user\",\"content\": self.query}]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    def mutate_state(self, state: State) -> State:\n",
    "\n",
    "        report_structure = self.run()\n",
    "        report_structure = remove_reasoning_from_output(report_structure)\n",
    "        report_structure = clean_json_tags(report_structure)\n",
    "\n",
    "        report_structure = json.loads(report_structure)\n",
    "\n",
    "        for paragraph in report_structure:\n",
    "            state.paragraphs.append(Paragraph(title=paragraph[\"title\"], content=paragraph[\"content\"]))\n",
    "\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa6b9c0-d47a-4297-bee6-f83ad8e4ac3a",
   "metadata": {},
   "source": [
    "### Agent to figure out the first search query for a given paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4403fcd-7233-468e-b902-db740044a445",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_schema_first_search = {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"title\": {\"type\": \"string\"},\n",
    "                \"content\": {\"type\": \"string\"}\n",
    "            }\n",
    "        }\n",
    "\n",
    "output_schema_first_search = {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"search_query\": {\"type\": \"string\"},\n",
    "                \"reasoning\": {\"type\": \"string\"}\n",
    "            }\n",
    "        }\n",
    "\n",
    "SYSTEM_PROMPT_FIRST_SEARCH = f\"\"\"\n",
    "You are a Deep Research assistan. You will be given a paragraph in a report, it's title and expected content in the following json schema definition:\n",
    "\n",
    "<INPUT JSON SCHEMA>\n",
    "{json.dumps(input_schema_first_search, indent=2)}\n",
    "</INPUT JSON SCHEMA>\n",
    "\n",
    "You can use a web search tool that takes a 'search_query' as parameter.\n",
    "Your job is to reflect on the topic and provide the most optimal web search query to enrich your current knowledge.\n",
    "Format the output in json with the following json schema definition:\n",
    "\n",
    "<OUTPUT JSON SCHEMA>\n",
    "{json.dumps(output_schema_first_search, indent=2)}\n",
    "</OUTPUT JSON SCHEMA>\n",
    "\n",
    "Make sure that the output is a json object with an output json schema defined above.\n",
    "Only return the json object, no explanation or additional text.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7725f6db-e1c5-48dd-b608-bc93c3a67010",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstSearchAgent:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.openai_client = openai.OpenAI(\n",
    "            api_key=config.SAMBANOVA_API_KEY,\n",
    "            base_url=config.SAMBANOVA_BASE_URL\n",
    "        )\n",
    "\n",
    "    def run(self, message) -> str:\n",
    "\n",
    "        response = self.openai_client.chat.completions.create(\n",
    "            model=config.LLM_REGULAR,\n",
    "            messages=[{\"role\": \"system\", \"content\": SYSTEM_PROMPT_FIRST_SEARCH},\n",
    "                      {\"role\":\"user\",\"content\": message}]\n",
    "        )\n",
    "\n",
    "        response = remove_reasoning_from_output(response.choices[0].message.content)\n",
    "        response = clean_json_tags(response)\n",
    "\n",
    "        response = json.loads(response)\n",
    "\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23acc12b-7106-4ccd-bcef-0a993da43b3e",
   "metadata": {},
   "source": [
    "### Agent to summarise search results of the first search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17175034-449b-4cc8-b536-614e130c9e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_schema_first_summary = {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"title\": {\"type\": \"string\"},\n",
    "                \"content\": {\"type\": \"string\"},\n",
    "                \"search_query\": {\"type\": \"string\"},\n",
    "                \"search_results\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\"type\": \"string\"}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "output_schema_first_summary = {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"paragraph_latest_state\": {\"type\": \"string\"}\n",
    "            }\n",
    "        }\n",
    "\n",
    "SYSTEM_PROMPT_FIRST_SUMMARY = f\"\"\"\n",
    "You are a Deep Research assistan. You will be given a search query, search results and the paragraph a report that you are researching following json schema definition:\n",
    "\n",
    "<INPUT JSON SCHEMA>\n",
    "{json.dumps(input_schema_first_summary, indent=2)}\n",
    "</INPUT JSON SCHEMA>\n",
    "\n",
    "Your job is to write the paragraph as a researcher using the search results to align with the paragraph topic and structure it properly to be included in the report.\n",
    "Format the output in json with the following json schema definition:\n",
    "\n",
    "<OUTPUT JSON SCHEMA>\n",
    "{json.dumps(output_schema_first_summary, indent=2)}\n",
    "</OUTPUT JSON SCHEMA>\n",
    "\n",
    "Make sure that the output is a json object with an output json schema defined above.\n",
    "Only return the json object, no explanation or additional text.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a3a30a-ecb6-4805-9e75-15d04da7036e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstSummaryAgent:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.openai_client = openai.OpenAI(\n",
    "            api_key=config.SAMBANOVA_API_KEY,\n",
    "            base_url=config.SAMBANOVA_BASE_URL\n",
    "        )\n",
    "\n",
    "    def run(self, message) -> str:\n",
    "\n",
    "        response = self.openai_client.chat.completions.create(\n",
    "            model=config.LLM_REGULAR,\n",
    "            messages=[{\"role\": \"system\", \"content\": SYSTEM_PROMPT_FIRST_SUMMARY},\n",
    "                      {\"role\":\"user\",\"content\": message}]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    def mutate_state(self, message: str, idx_paragraph: int, state: State) -> State:\n",
    "\n",
    "        summary = self.run(message)\n",
    "        summary = remove_reasoning_from_output(summary)\n",
    "        summary = clean_json_tags(summary)\n",
    "        \n",
    "        try:\n",
    "            summary = json.loads(summary)\n",
    "        except JSONDecodeError:\n",
    "            summary = {\"paragraph_latest_state\": summary}\n",
    "\n",
    "        state.paragraphs[idx_paragraph].research.latest_summary = summary[\"paragraph_latest_state\"]\n",
    "\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f41e15-706b-460f-9bdf-c38159a20d16",
   "metadata": {},
   "source": [
    "### Agent to Reflect on the latest state of the paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26a3828-6975-44d3-9188-02a17bc5617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_schema_reflection = {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"title\": {\"type\": \"string\"},\n",
    "                \"content\": {\"type\": \"string\"},\n",
    "                \"paragraph_latest_state\": {\"type\": \"string\"}\n",
    "            }\n",
    "        }\n",
    "\n",
    "output_schema_reflection = {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"search_query\": {\"type\": \"string\"},\n",
    "                \"reasoning\": {\"type\": \"string\"}\n",
    "            }\n",
    "        }\n",
    "\n",
    "SYSTEM_PROMPT_REFLECTION = f\"\"\"\n",
    "You are a Deep Research assistan. You are responsible for constructing comprehensife paragraphs for a research report. You will be provided paragraph title and planned content summary, also the latest state of the paragraph that you have already created all in the following json schema definition:\n",
    "\n",
    "<INPUT JSON SCHEMA>\n",
    "{json.dumps(input_schema_reflection, indent=2)}\n",
    "</INPUT JSON SCHEMA>\n",
    "\n",
    "You can use a web search tool that takes a 'search_query' as parameter.\n",
    "Your job is to reflect on the current state of the paragraph text and think if you havent missed some critical aspect of the topic and provide the most optimal web search query to enrich the latest state.\n",
    "Format the output in json with the following json schema definition:\n",
    "\n",
    "<OUTPUT JSON SCHEMA>\n",
    "{json.dumps(output_schema_reflection, indent=2)}\n",
    "</OUTPUT JSON SCHEMA>\n",
    "\n",
    "Make sure that the output is a json object with an output json schema defined above.\n",
    "Only return the json object, no explanation or additional text.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2e65b7-fab6-4dfd-b12f-b269dd815944",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflectionAgent:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.openai_client = openai.OpenAI(\n",
    "            api_key=config.SAMBANOVA_API_KEY,\n",
    "            base_url=config.SAMBANOVA_BASE_URL\n",
    "        )\n",
    "\n",
    "    def run(self, message) -> str:\n",
    "\n",
    "        response = self.openai_client.chat.completions.create(\n",
    "            model=config.LLM_REGULAR,\n",
    "            messages=[{\"role\": \"system\", \"content\": SYSTEM_PROMPT_REFLECTION},\n",
    "                      {\"role\":\"user\",\"content\": message}]\n",
    "        )\n",
    "\n",
    "        response = remove_reasoning_from_output(response.choices[0].message.content)\n",
    "        response = clean_json_tags(response)\n",
    "        response = json.loads(response)\n",
    "\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ce5813-bd97-4da0-a36b-16eb73536a51",
   "metadata": {},
   "source": [
    "### Agent to summarise search results after Reflection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7299fe-ed97-4210-8fe1-21ebbdaa395a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_schema_reflection_summary = {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"title\": {\"type\": \"string\"},\n",
    "                \"content\": {\"type\": \"string\"},\n",
    "                \"search_query\": {\"type\": \"string\"},\n",
    "                \"search_results\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\"type\": \"string\"}\n",
    "                },\n",
    "                \"paragraph_latest_state\": {\"type\": \"string\"}\n",
    "            }\n",
    "        }\n",
    "\n",
    "output_schema_reflection_summary = {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"updated_paragraph_latest_state\": {\"type\": \"string\"}\n",
    "            }\n",
    "        }\n",
    "\n",
    "SYSTEM_PROMPT_REFLECTION_SUMMARY = f\"\"\"\n",
    "You are a Deep Research assistan.\n",
    "You will be given a search query, search results, paragraph title and expected content for the paragraph in a report that you are researching.\n",
    "You are iterating on the paragraph and the latest state of the paragraph is also provided.\n",
    "The data will be in the following json schema definition:\n",
    "\n",
    "<INPUT JSON SCHEMA>\n",
    "{json.dumps(input_schema_reflection_summary, indent=2)}\n",
    "</INPUT JSON SCHEMA>\n",
    "\n",
    "Your job is to enrich the current latest state of the paragraph with the search results considering expected content.\n",
    "Do not remove key information from the latest state and try to enrich it, only add information that is missing.\n",
    "Structure the paragraph properly to be included in the report.\n",
    "Format the output in json with the following json schema definition:\n",
    "\n",
    "<OUTPUT JSON SCHEMA>\n",
    "{json.dumps(output_schema_reflection_summary, indent=2)}\n",
    "</OUTPUT JSON SCHEMA>\n",
    "\n",
    "Make sure that the output is a json object with an output json schema defined above.\n",
    "Only return the json object, no explanation or additional text.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a661e117-6e7d-497f-ab4c-a9a6e13709c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflectionSummaryAgent:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.openai_client = openai.OpenAI(\n",
    "            api_key=config.SAMBANOVA_API_KEY,\n",
    "            base_url=config.SAMBANOVA_BASE_URL\n",
    "        )\n",
    "\n",
    "    def run(self, message) -> str:\n",
    "\n",
    "        response = self.openai_client.chat.completions.create(\n",
    "            model=config.LLM_REGULAR,\n",
    "            messages=[{\"role\": \"system\", \"content\": SYSTEM_PROMPT_REFLECTION_SUMMARY},\n",
    "                      {\"role\":\"user\",\"content\": message}]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    def mutate_state(self, message: str, idx_paragraph: int, state: State) -> State:\n",
    "\n",
    "        summary = self.run(message)\n",
    "        summary = remove_reasoning_from_output(summary)\n",
    "        summary = clean_json_tags(summary)\n",
    "\n",
    "        try:\n",
    "            summary = json.loads(summary)\n",
    "        except JSONDecodeError:\n",
    "            summary = {\"updated_paragraph_latest_state\": summary}\n",
    "\n",
    "        state.paragraphs[idx_paragraph].research.latest_summary = summary[\"updated_paragraph_latest_state\"]\n",
    "\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf9e53a-5455-463c-82aa-60261bd7762d",
   "metadata": {},
   "source": [
    "### Agent to summarise results and produce the formatted report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd8e72d-b050-4e6e-a860-65caeb627049",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_schema_report_formatting = {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"title\": {\"type\": \"string\"},\n",
    "                    \"paragraph_latest_state\": {\"type\": \"string\"}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "SYSTEM_PROMPT_REPORT_FORMATTING = f\"\"\"\n",
    "You are a Deep Research assistan. You have already performed the research and construted final versions of all paragraphs in the report.\n",
    "You will get the data in the following json format:\n",
    "\n",
    "<INPUT JSON SCHEMA>\n",
    "{json.dumps(input_schema_report_formatting, indent=2)}\n",
    "</INPUT JSON SCHEMA>\n",
    "\n",
    "Your job is to format the Report nicely and return it in MarkDown.\n",
    "If Conclusion paragraph is not present, add it to the end of the report from the latest state of the other paragraphs.\n",
    "Use titles of the paragraphs to create a title for the report.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43924fb3-fe3d-4c17-b1e8-6488b4c35839",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReportFormattingAgent:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.openai_client = openai.OpenAI(\n",
    "            api_key=config.SAMBANOVA_API_KEY,\n",
    "            base_url=config.SAMBANOVA_BASE_URL\n",
    "        )\n",
    "\n",
    "    def run(self, message) -> str:\n",
    "\n",
    "        response = self.openai_client.chat.completions.create(\n",
    "            model=config.LLM_REASONING,\n",
    "            messages=[{\"role\": \"system\", \"content\": SYSTEM_PROMPT_REPORT_FORMATTING},\n",
    "                      {\"role\":\"user\",\"content\": message}]\n",
    "        )\n",
    "        summary = response.choices[0].message.content\n",
    "        summary = remove_reasoning_from_output(summary)\n",
    "        summary = clean_markdown_tags(summary)\n",
    "        \n",
    "        return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068919b8-3798-4896-80c1-9863ccbc9b6e",
   "metadata": {},
   "source": [
    "## The Topology of the System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34692f4-b385-4461-912d-65c7038befec",
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE = State()\n",
    "QUERY=\"Tell me something interesting about human species\"\n",
    "NUM_REFLECTIONS = 2\n",
    "NUM_RESULTS_PER_SEARCH = 3\n",
    "CAP_SEARCH_LENGTH = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46750f1-73a3-4320-b246-94a62f7c304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_structure_agent = ReportStructureAgent(topic)\n",
    "\n",
    "_ = report_structure_agent.mutate_state(STATE)\n",
    "\n",
    "first_search_agent = FirstSearchAgent()\n",
    "first_summary_agent = FirstSummaryAgent()\n",
    "reflection_agent = ReflectionAgent()\n",
    "reflection_summary_agent = ReflectionSummaryAgent()\n",
    "report_formatting_agent = ReportFormattingAgent()\n",
    "\n",
    "print(f\"Total Number of Paragraphs: {len(STATE.paragraphs)}\")\n",
    "\n",
    "idx = 1\n",
    "\n",
    "for paragraph in STATE.paragraphs:\n",
    "\n",
    "    print(f\"\\nParagraph {idx}: {paragraph.title}\")\n",
    "\n",
    "    idx += 1\n",
    "\n",
    "\n",
    "################## Iterate through paragraphs ##################\n",
    "\n",
    "for j in range(len(STATE.paragraphs)):\n",
    "\n",
    "    print(f\"\\n\\n==============Paragraph: {j+1}==============\\n\")\n",
    "    print(f\"=============={STATE.paragraphs[j].title}==============\\n\")\n",
    "\n",
    "    ################## First Search ##################\n",
    "    \n",
    "    message = json.dumps(\n",
    "        {\n",
    "            \"title\": STATE.paragraphs[j].title, \n",
    "            \"content\": STATE.paragraphs[j].content\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    output = first_search_agent.run(message)\n",
    "    \n",
    "    search_results = tavily_search(output[\"search_query\"], max_results=NUM_RESULTS_PER_SEARCH)\n",
    "    \n",
    "    _ = update_state_with_search_results(search_results, j, STATE)\n",
    "    \n",
    "    ################## First Search Summary ##################\n",
    "    \n",
    "    message = {\n",
    "        \"title\": STATE.paragraphs[j].title,\n",
    "        \"content\": STATE.paragraphs[j].content,\n",
    "        \"search_query\": search_results[\"query\"],\n",
    "        \"search_results\": [result[\"raw_content\"][0:CAP_SEARCH_LENGTH] for result in search_results[\"results\"] if result[\"raw_content\"]]\n",
    "    }\n",
    "    \n",
    "    \n",
    "    _ = first_summary_agent.mutate_state(message=json.dumps(message), idx_paragraph=j, state=STATE)\n",
    "    \n",
    "    ################## Run NUM_REFLECTIONS Reflection steps ##################\n",
    "    \n",
    "    for i in range(NUM_REFLECTIONS):\n",
    "    \n",
    "        print(f\"Running reflection: {i+1}\")\n",
    "\n",
    "        ################## Reflection Step ##################\n",
    "    \n",
    "        message = {\"paragraph_latest_state\": STATE.paragraphs[j].research.latest_summary,\n",
    "                \"title\": STATE.paragraphs[j].title,\n",
    "                \"content\": STATE.paragraphs[j].content}\n",
    "        \n",
    "        output = reflection_agent.run(message=json.dumps(message))\n",
    "\n",
    "        ################## Reflection Search ##################\n",
    "        \n",
    "        search_results = tavily_search(output[\"search_query\"])\n",
    "        \n",
    "        _ = update_state_with_search_results(search_results, j, STATE)\n",
    "\n",
    "        ################## Reflection Search Summary ##################\n",
    "        \n",
    "        message = {\n",
    "            \"title\": STATE.paragraphs[j].title,\n",
    "            \"content\": STATE.paragraphs[j].content,\n",
    "            \"search_query\": search_results[\"query\"],\n",
    "            \"search_results\": [result[\"raw_content\"][0:20000] for result in search_results[\"results\"] if result[\"raw_content\"]],\n",
    "            \"paragraph_latest_state\": STATE.paragraphs[j].research.latest_summary\n",
    "        }\n",
    "        \n",
    "        _ = reflection_summary_agent.mutate_state(message=json.dumps(message), idx_paragraph=j, state=STATE)\n",
    "\n",
    "################## Generate Final Report ##################\n",
    "\n",
    "report_data = [{\"title\": paragraph.title, \"paragraph_latest_state\": paragraph.research.latest_summary} for paragraph in STATE.paragraphs]\n",
    "\n",
    "final_report = report_formatting_agent.run(json.dumps(report_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d303a7-4939-4af9-af83-45847b9a95db",
   "metadata": {},
   "source": [
    "### Render the final Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f53788-7446-4408-9b48-a2cf98222c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(final_report))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
